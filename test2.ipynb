{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ead776-ab98-4d46-9573-48fdeb09a6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8928ff1-07b8-4e7a-b17f-13b15d724c47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc62ba46-9d03-4063-af8b-bf82f6a940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.models.models import SimpleCNN,EPIMind, init_weights\n",
    "import torchsummary\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e62db0-7c59-42e5-a690-5bacd0595753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SimpleCNN(encode_method=\"dna2vec\",concat_reverse=False,\n",
    "                  init_method=\"xavier_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78508fc-48c9-41b7-bce9-1846ab3d6b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = EPIMind(concat_reverse=False,embedding_dim=100,\n",
    "                 num_heads=8, num_layers=8, num_hiddens=128, ffn_num_hiddens=256,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a661327-1328-433e-88b6-bdcd4632eefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9a7bb4e-473f-4840-8f61-caae2fc98b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "summary() got an unexpected keyword argument 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorchsummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters_count:\u001b[39m\u001b[38;5;124m'\u001b[39m,count_parameters(model))\n",
      "\u001b[0;31mTypeError\u001b[0m: summary() got an unexpected keyword argument 'tgt'"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, (4000, 200),(4000, 200))\n",
    "print('parameters_count:',count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e741373-ab87-4658-bf4b-02de74075200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from thop import profile\n",
    "model = SimpleCNN(encode_method=\"dna2vec\",concat_reverse=False,\n",
    "                  init_method=\"xavier_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afad174c-e514-4df9-b144-20345b3b8618",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module PositionalEncoding is treated as a zero-op.\n",
      "Warning: module DotProductAttention is treated as a zero-op.\n",
      "Warning: module MultiHeadAttention is treated as a zero-op.\n",
      "Warning: module AddNorm is treated as a zero-op.\n",
      "Warning: module PositionWiseFFN is treated as a zero-op.\n",
      "Warning: module EncoderBlock is treated as a zero-op.\n",
      "Warning: module TransformerEncoder is treated as a zero-op.\n",
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Softmax is treated as a zero-op.\n",
      "Warning: module EPIMind is treated as a zero-op.\n",
      "EPIMind(\n",
      "  4.25 M, 100.000% Params, 3.05 GMac, 100.000% MACs, \n",
      "  (enhancer_conv): Sequential(\n",
      "    460.93 k, 10.844% Params, 1.83 GMac, 60.007% MACs, \n",
      "    (0): Conv1d(460.93 k, 10.844% Params, 1.83 GMac, 59.990% MACs, 100, 128, kernel_size=(36,), stride=(1,), padding=valid)\n",
      "    (1): ReLU(0, 0.000% Params, 507.52 KMac, 0.017% MACs, )\n",
      "  )\n",
      "  (promoter_conv): Sequential(\n",
      "    460.93 k, 10.844% Params, 905.98 MMac, 29.739% MACs, \n",
      "    (0): Conv1d(460.93 k, 10.844% Params, 905.72 MMac, 29.730% MACs, 100, 128, kernel_size=(36,), stride=(1,), padding=valid)\n",
      "    (1): ReLU(0, 0.000% Params, 251.52 KMac, 0.008% MACs, )\n",
      "  )\n",
      "  (enhancer_maxpool): Sequential(\n",
      "    0, 0.000% Params, 507.52 KMac, 0.017% MACs, \n",
      "    (0): MaxPool1d(0, 0.000% Params, 507.52 KMac, 0.017% MACs, kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (promoter_maxpool): Sequential(\n",
      "    0, 0.000% Params, 251.52 KMac, 0.008% MACs, \n",
      "    (0): MaxPool1d(0, 0.000% Params, 251.52 KMac, 0.008% MACs, kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (transformer_encoder_enhancer): TransformerEncoder(\n",
      "    1.86 M, 43.822% Params, 208.43 MMac, 6.842% MACs, \n",
      "    (pos_encoding): PositionalEncoding(\n",
      "      0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "    )\n",
      "    (blks): Sequential(\n",
      "      1.86 M, 43.822% Params, 208.43 MMac, 6.842% MACs, \n",
      "      (block0): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block1): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block2): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block3): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block4): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block5): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block6): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block7): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_encoder_promoter): TransformerEncoder(\n",
      "    1.45 M, 34.185% Params, 103.16 MMac, 3.386% MACs, \n",
      "    (pos_encoding): PositionalEncoding(\n",
      "      0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "    )\n",
      "    (blks): Sequential(\n",
      "      1.45 M, 34.185% Params, 103.16 MMac, 3.386% MACs, \n",
      "      (block0): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block1): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block2): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block3): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block4): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block5): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block6): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block7): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (global_maxpool1): AdaptiveMaxPool1d(0, 0.000% Params, 25.34 KMac, 0.001% MACs, output_size=1)\n",
      "  (global_maxpool2): AdaptiveMaxPool1d(0, 0.000% Params, 12.54 KMac, 0.000% MACs, output_size=1)\n",
      "  (flatten): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (dense): Sequential(\n",
      "    12.85 k, 0.302% Params, 12.9 KMac, 0.000% MACs, \n",
      "    (0): Linear(12.85 k, 0.302% Params, 12.85 KMac, 0.000% MACs, in_features=256, out_features=50, bias=True)\n",
      "    (1): ReLU(0, 0.000% Params, 50.0 Mac, 0.000% MACs, )\n",
      "  )\n",
      "  (dense1): Sequential(\n",
      "    102, 0.002% Params, 102.0 Mac, 0.000% MACs, \n",
      "    (0): Linear(102, 0.002% Params, 102.0 Mac, 0.000% MACs, in_features=50, out_features=2, bias=True)\n",
      "    (1): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "import re\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "  # net = models.densenet161()\n",
    "  macs, params = get_model_complexity_info(model, (6000, 200), as_strings=True,\n",
    "                                           print_per_layer_stat=True, verbose=True)\n",
    "flops = eval(re.findall(r'([\\d.]+)', macs)[0])*2 \n",
    "# Extract the unit \n",
    "flops_unit = re.findall(r'([A-Za-z]+)', macs)[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d428cb-2b6c-4ac3-98bd-2fd09bb06522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity: 3.05 GMac\n",
      "Computational complexity: 6.1 GFlops\n",
      "Number of parameters: 4.25 M  \n"
     ]
    }
   ],
   "source": [
    "print('Computational complexity: {:<8}'.format(macs)) \n",
    "print('Computational complexity: {} {}Flops'.format(flops, flops_unit)) \n",
    "print('Number of parameters: {:<8}'.format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ff9529-4514-4391-81d2-41b18a3e9850",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.transformer.Transformer"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ef3b79-0c35-46a5-8e19-e9b76c0d50f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    " \n",
    "#第一种方式\n",
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return torch.max_pool1d(x,kernel_size=x.shape[2])\n",
    " \n",
    "a = torch.tensor(np.arange(24),dtype=torch.float).view(2,3,4).cuda()\n",
    "print(a.shape)\n",
    "gmp1 = GlobalMaxPool1d()\n",
    "print(gmp1(a).shape)\n",
    " \n",
    " \n",
    "#第二种方式\n",
    "gmp2 = torch.nn.AdaptiveMaxPool1d(1)\n",
    "print(gmp2(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19524d50-020d-4fcf-8eb0-b35e8f46cbe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "print(flatten(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b004ea11-0d9d-44e4-a553-6e5e3906e302",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'reduce_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'reduce_sum'"
     ]
    }
   ],
   "source": [
    "torch.reduce_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90cd4cee-7730-4a99-912b-613ac9de6544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpaddle\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paddle'"
     ]
    }
   ],
   "source": [
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c907f2d-fb7d-4ac7-b0af-c1fe0a990438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
