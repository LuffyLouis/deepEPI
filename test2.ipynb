{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ead776-ab98-4d46-9573-48fdeb09a6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8928ff1-07b8-4e7a-b17f-13b15d724c47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc62ba46-9d03-4063-af8b-bf82f6a940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.models.models import SimpleCNN,EPIMind, init_weights\n",
    "import torchsummary\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e62db0-7c59-42e5-a690-5bacd0595753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SimpleCNN(encode_method=\"dna2vec\",concat_reverse=False,\n",
    "                  init_method=\"xavier_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78508fc-48c9-41b7-bce9-1846ab3d6b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = EPIMind(concat_reverse=False,embedding_dim=100,\n",
    "                 num_heads=8, num_layers=8, num_hiddens=128, ffn_num_hiddens=256,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a661327-1328-433e-88b6-bdcd4632eefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea32b3df-d73e-4a33-b1b2-b3bb811908af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "a = model(torch.randn((1, 32, 512)),torch.randn((1, 32, 512)))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9a7bb4e-473f-4840-8f61-caae2fc98b06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorchsummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters_count:\u001b[39m\u001b[38;5;124m'\u001b[39m,count_parameters(model))\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torch/nn/modules/transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    147\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    148\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torch/nn/modules/transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m make_causal\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 315\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    318\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torch/nn/modules/transformer.py:591\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    589\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    592\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torch/nn/modules/transformer.py:599\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    598\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 599\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torchsummary/torchsummary.py:22\u001b[0m, in \u001b[0;36msummary.<locals>.register_hook.<locals>.hook\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     20\u001b[0m summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m---> 22\u001b[0m     summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(o\u001b[38;5;241m.\u001b[39msize())[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/miniconda3/envs/deepEPI/lib/python3.8/site-packages/torchsummary/torchsummary.py:23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     22\u001b[0m     summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 23\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, [(32, 512),(32, 512)],batch_size=16,device=\"cpu\")\n",
    "print('parameters_count:',count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e741373-ab87-4658-bf4b-02de74075200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from thop import profile\n",
    "model = SimpleCNN(encode_method=\"dna2vec\",concat_reverse=False,\n",
    "                  init_method=\"xavier_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afad174c-e514-4df9-b144-20345b3b8618",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module PositionalEncoding is treated as a zero-op.\n",
      "Warning: module DotProductAttention is treated as a zero-op.\n",
      "Warning: module MultiHeadAttention is treated as a zero-op.\n",
      "Warning: module AddNorm is treated as a zero-op.\n",
      "Warning: module PositionWiseFFN is treated as a zero-op.\n",
      "Warning: module EncoderBlock is treated as a zero-op.\n",
      "Warning: module TransformerEncoder is treated as a zero-op.\n",
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Softmax is treated as a zero-op.\n",
      "Warning: module EPIMind is treated as a zero-op.\n",
      "EPIMind(\n",
      "  4.25 M, 100.000% Params, 3.05 GMac, 100.000% MACs, \n",
      "  (enhancer_conv): Sequential(\n",
      "    460.93 k, 10.844% Params, 1.83 GMac, 60.007% MACs, \n",
      "    (0): Conv1d(460.93 k, 10.844% Params, 1.83 GMac, 59.990% MACs, 100, 128, kernel_size=(36,), stride=(1,), padding=valid)\n",
      "    (1): ReLU(0, 0.000% Params, 507.52 KMac, 0.017% MACs, )\n",
      "  )\n",
      "  (promoter_conv): Sequential(\n",
      "    460.93 k, 10.844% Params, 905.98 MMac, 29.739% MACs, \n",
      "    (0): Conv1d(460.93 k, 10.844% Params, 905.72 MMac, 29.730% MACs, 100, 128, kernel_size=(36,), stride=(1,), padding=valid)\n",
      "    (1): ReLU(0, 0.000% Params, 251.52 KMac, 0.008% MACs, )\n",
      "  )\n",
      "  (enhancer_maxpool): Sequential(\n",
      "    0, 0.000% Params, 507.52 KMac, 0.017% MACs, \n",
      "    (0): MaxPool1d(0, 0.000% Params, 507.52 KMac, 0.017% MACs, kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (promoter_maxpool): Sequential(\n",
      "    0, 0.000% Params, 251.52 KMac, 0.008% MACs, \n",
      "    (0): MaxPool1d(0, 0.000% Params, 251.52 KMac, 0.008% MACs, kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (transformer_encoder_enhancer): TransformerEncoder(\n",
      "    1.86 M, 43.822% Params, 208.43 MMac, 6.842% MACs, \n",
      "    (pos_encoding): PositionalEncoding(\n",
      "      0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "    )\n",
      "    (blks): Sequential(\n",
      "      1.86 M, 43.822% Params, 208.43 MMac, 6.842% MACs, \n",
      "      (block0): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block1): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block2): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block3): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block4): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block5): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block6): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block7): EncoderBlock(\n",
      "        232.83 k, 5.478% Params, 26.05 MMac, 0.855% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 12.98 MMac, 0.426% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 3.24 MMac, 0.106% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 13.03 MMac, 0.428% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 6.49 MMac, 0.213% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 50.69 KMac, 0.002% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 6.49 MMac, 0.213% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(50.69 k, 1.193% Params, 25.34 KMac, 0.001% MACs, (198, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_encoder_promoter): TransformerEncoder(\n",
      "    1.45 M, 34.185% Params, 103.16 MMac, 3.386% MACs, \n",
      "    (pos_encoding): PositionalEncoding(\n",
      "      0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "    )\n",
      "    (blks): Sequential(\n",
      "      1.45 M, 34.185% Params, 103.16 MMac, 3.386% MACs, \n",
      "      (block0): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block1): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block2): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block3): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block4): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block5): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block6): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (block7): EncoderBlock(\n",
      "        181.63 k, 4.273% Params, 12.9 MMac, 0.423% MACs, \n",
      "        (attention): MultiHeadAttention(\n",
      "          65.54 k, 1.542% Params, 6.42 MMac, 0.211% MACs, \n",
      "          (attention): DotProductAttention(\n",
      "            0, 0.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (W_q): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_k): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_v): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "          (W_o): Linear(16.38 k, 0.385% Params, 1.61 MMac, 0.053% MACs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (addnorm1): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ffn): PositionWiseFFN(\n",
      "          65.92 k, 1.551% Params, 6.45 MMac, 0.212% MACs, \n",
      "          (dense1): Linear(33.02 k, 0.777% Params, 3.21 MMac, 0.105% MACs, in_features=128, out_features=256, bias=True)\n",
      "          (relu): ReLU(0, 0.000% Params, 25.09 KMac, 0.001% MACs, )\n",
      "          (dense2): Linear(32.9 k, 0.774% Params, 3.21 MMac, 0.105% MACs, in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (addnorm2): AddNorm(\n",
      "          25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, \n",
      "          (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      "          (ln): LayerNorm(25.09 k, 0.590% Params, 12.54 KMac, 0.000% MACs, (98, 128), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (global_maxpool1): AdaptiveMaxPool1d(0, 0.000% Params, 25.34 KMac, 0.001% MACs, output_size=1)\n",
      "  (global_maxpool2): AdaptiveMaxPool1d(0, 0.000% Params, 12.54 KMac, 0.000% MACs, output_size=1)\n",
      "  (flatten): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (dense): Sequential(\n",
      "    12.85 k, 0.302% Params, 12.9 KMac, 0.000% MACs, \n",
      "    (0): Linear(12.85 k, 0.302% Params, 12.85 KMac, 0.000% MACs, in_features=256, out_features=50, bias=True)\n",
      "    (1): ReLU(0, 0.000% Params, 50.0 Mac, 0.000% MACs, )\n",
      "  )\n",
      "  (dense1): Sequential(\n",
      "    102, 0.002% Params, 102.0 Mac, 0.000% MACs, \n",
      "    (0): Linear(102, 0.002% Params, 102.0 Mac, 0.000% MACs, in_features=50, out_features=2, bias=True)\n",
      "    (1): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "import re\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "  # net = models.densenet161()\n",
    "  macs, params = get_model_complexity_info(model, (6000, 200), as_strings=True,\n",
    "                                           print_per_layer_stat=True, verbose=True)\n",
    "flops = eval(re.findall(r'([\\d.]+)', macs)[0])*2 \n",
    "# Extract the unit \n",
    "flops_unit = re.findall(r'([A-Za-z]+)', macs)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d428cb-2b6c-4ac3-98bd-2fd09bb06522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity: 3.05 GMac\n",
      "Computational complexity: 6.1 GFlops\n",
      "Number of parameters: 4.25 M  \n"
     ]
    }
   ],
   "source": [
    "print('Computational complexity: {:<8}'.format(macs)) \n",
    "print('Computational complexity: {} {}Flops'.format(flops, flops_unit)) \n",
    "print('Number of parameters: {:<8}'.format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ef3b79-0c35-46a5-8e19-e9b76c0d50f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    " \n",
    "#第一种方式\n",
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return torch.max_pool1d(x,kernel_size=x.shape[2])\n",
    " \n",
    "a = torch.tensor(np.arange(24),dtype=torch.float).view(2,3,4).cuda()\n",
    "print(a.shape)\n",
    "gmp1 = GlobalMaxPool1d()\n",
    "print(gmp1(a).shape)\n",
    " \n",
    " \n",
    "#第二种方式\n",
    "gmp2 = torch.nn.AdaptiveMaxPool1d(1)\n",
    "print(gmp2(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19524d50-020d-4fcf-8eb0-b35e8f46cbe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "print(flatten(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c907f2d-fb7d-4ac7-b0af-c1fe0a990438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
